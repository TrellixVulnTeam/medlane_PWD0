{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12801it [00:00, 716906.59it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = open('./data_tmp/train(12809).txt', 'r', encoding='utf-8').read()\n",
    "txt = txt.split('\\n\\n')\n",
    "srcs = pickle.load(open('./data_tmp/train_tar_sts.pkl','rb'))\n",
    "train_input = []\n",
    "train_output = []\n",
    "for para, src in tqdm(zip(txt,srcs)):\n",
    "    sentences = para.split('\\n')\n",
    "    st1 = sentences[1]\n",
    "    train_input.append(src)\n",
    "    train_output.append('[CLS] '+st1+' [SEP]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] Stage 2 ulcers : On sacrum and scrotum . [SEP]', '[CLS] Labs were significant for day(s) - dimer 6218 , troponin 0 . 05 , creatinine 1 . 4 , Hco3 20 and wbc 11 . 3 . [SEP]', '[CLS] They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation . [SEP]', '[CLS] Serum toxicology screens were negative for benzodiazepenes , barbituates and tricyclic antidepressants . [SEP]', '[CLS] Known surname was admitted to the Neurosurgery service after a study revealed a break in her [unused4] catheter at the level of her neck . [SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(srcs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(sentence, cudic):\n",
    "    sentence = sentence[2:]\n",
    "    text = re.sub('\\[[^\\[\\]]*\\]', '', sentence)\n",
    "    pairs = re.findall('[^\\[\\] ]+\\[[^\\[\\]]+\\]', sentence)\n",
    "    for pair in pairs:\n",
    "        pair = re.split('[\\[\\]]', pair)\n",
    "        cudic[pair[0]] = pair[1]\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for wid, word in enumerate(words):\n",
    "        if word in cudic.keys():\n",
    "            words[wid] = cudic[word]\n",
    "    return ' '.join(words), cudic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2031it [00:01, 1799.89it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = open('./data_tmp/test(2030).txt', 'r', encoding='utf-8').read()\n",
    "txt = txt.split('\\n\\n')\n",
    "srcs = pickle.load(open('./data_tmp/test_tar_sts.pkl','rb'))\n",
    "test_input = []\n",
    "test_output = []\n",
    "test_tars = []\n",
    "cudics = []\n",
    "for para, src in tqdm(zip(txt, srcs)):\n",
    "    sentences = para.split('\\n')\n",
    "    test_input.append(src)\n",
    "    cudic = {}\n",
    "    test_tar = []\n",
    "    \n",
    "    for i, st in enumerate(sentences):\n",
    "        if i==0:\n",
    "            continue\n",
    "        else:\n",
    "            st_, cu = to_sentence(st,cudic)\n",
    "            cudic.update(cu)\n",
    "            test_tar.append(st_)\n",
    "            if i==2:\n",
    "                test_output.append(st_)\n",
    "    test_tars.append(test_tar)\n",
    "    cudics.append(cudic)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She was intubated and was waken up after 10-22 minutes of unresponsive pulse arrest .\n"
     ]
    }
   ],
   "source": [
    "print(test_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cudics,open('./data_tmp/test_cudics.pkl','wb'))\n",
    "pickle.dump(test_tars,open('./data_tmp/test_tars.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] Stage 2 ulcers : On sacrum and scrotum . [SEP]', '[CLS] Labs were significant for day(s) - dimer 6218 , troponin 0 . 05 , creatinine 1 . 4 , Hco3 20 and wbc 11 . 3 . [SEP]', '[CLS] They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation . [SEP]', '[CLS] Serum toxicology screens were negative for benzodiazepenes , barbituates and tricyclic antidepressants . [SEP]', '[CLS] Known surname was admitted to the Neurosurgery service after a study revealed a break in her [unused4] catheter at the level of her neck . [SEP]']\n",
      "['[CLS] Stage 2 break in skin : On sacrum and scrotum . [SEP]', '[CLS] Labs were significant for d-dimer 6218 , troponin 0.05 , creatinine 1.4 , Bicarbonate 20 and white blood cells 11.3 . [SEP]', '[CLS] They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation . [SEP]', '[CLS] Serum toxicology test were negative for benzodiazepenes , barbituates and tricyclic antidepressants . [SEP]', '[CLS] Patient was admitted to the Neurosurgery service after a study revealed a break in her shunt catheter at the level of her neck . [SEP]']\n",
      "['[CLS] Weaned off vent to Continuous positive airway pressure and was extubated in the afternoon on 9 - 2 by the lung team . [SEP]', '[CLS] She was intubated and was resuscitated after 10 - 22 minutes of [unused4] arrest . [SEP]', '[CLS] He was given succinyl choline and etomidate for intubation . [SEP]', '[CLS] Had posterior, after [unused4] [unused4] [unused4] and was transfused with appropriate effect . [SEP]', '[CLS] The patient was maintained on logroll precautions until [unused4] spine films were obtained and read as negative . [SEP]']\n",
      "['Weaned off vent to continuous positive airway pressure and was extubated in the afternoon on 9-2 by the lung specialist team .', 'She was intubated and was waken up after 10-22 minutes of unresponsive pulse arrest .', 'He was given succinyl choline and etomidate for intubation .', 'Had post-op Lack of enough healthy red blood cells and was transfused with appropriate effect .', 'The patient was maintained on logroll diseases prevention until TL spine films were obtained and read as negative .']\n",
      "[{'CPAP': 'continuous positive airway pressure'}, {'PEA': 'pulseless electrical activity'}, {}, {}, {}]\n"
     ]
    }
   ],
   "source": [
    "print(train_input[:5])\n",
    "print(train_output[:5])\n",
    "print(test_input[:5])\n",
    "print(test_output[:5])\n",
    "print(cudics[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [unused4] is a 89 years old male with sick sinus syndrome [unused4] extension [unused4] pacer , admitted to Neurosurgery with [unused4] with acute breathing distress extension [unused4] on the floor . [SEP]\n",
      "patient is a 89 year old male with irregular heart rhythms after pacer , admitted to Neurosurgery with brain blood collects with acute respiratory distress/ wheezing on the floor .\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,(u,v) in enumerate(zip(test_input,test_output)):\n",
    "    if 'patient is a 89 year old' in v:\n",
    "        print(u)\n",
    "        print(v)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_tmp/train_input.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_input,f)\n",
    "with open(\"./data_tmp/train_output.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_output,f)\n",
    "with open(\"./data_tmp/test_input.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_input,f)\n",
    "with open(\"./data_tmp/test_output.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_output,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:06<00:00, 2035.21it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:00<00:00, 2132.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from importlib import import_module\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "model_name = 'bert'\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(16)\n",
    "tokenizer = config.tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = False)\n",
    "\n",
    "def tokenize(input_data, output_data):\n",
    "    ids = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for i in tqdm(range(len(input_data))):\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                                 input_data[i],                      # Sentence to encode.\n",
    "#                                 add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "#                                 max_length = 64,           # Pad & truncate all sentences.\n",
    "#                                 pad_to_max_length = True,\n",
    "#                                 return_attention_mask = True,   # Construct attn. masks.\n",
    "#                                 return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                            )\n",
    "#         words = nltk.word_tokenize(input_data[i])\n",
    "#         tokenized_sentence = []\n",
    "#         for word in words:\n",
    "#             tokenized_word = tokenizer.tokenize(word)\n",
    "#             tokenized_sentence.extend(tokenized_word)\n",
    "#         ids.append(tokenized_sentence)\n",
    "        \n",
    "#         words = nltk.word_tokenize(output_data[i])\n",
    "#         tokenized_sentence = []\n",
    "#         for word in words:\n",
    "#             tokenized_word = tokenizer.tokenize(word)\n",
    "#             tokenized_sentence.extend(tokenized_word)\n",
    "#         labels.append(tokenized_sentence)\n",
    "        words = tokenizer.tokenize(input_data[i])\n",
    "        ids.append(words)\n",
    "        words = tokenizer.tokenize(output_data[i])\n",
    "        labels.append(words)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in ids],\n",
    "                          maxlen=64, dtype=\"long\", value=0,\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    labels = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in labels],\n",
    "                          maxlen=64, dtype=\"long\", value=0,\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "        \n",
    "    return ids, labels\n",
    "    \n",
    "#         ids.append(encoded_dict['input_ids'])\n",
    "#         masks.append(encoded_dict['attention_mask'])\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                                 output_data[i],add_special_tokens = False, max_length = 64, pad_to_max_length = True, return_attention_mask = True,\n",
    "#                                 return_tensors = 'pt',)\n",
    "#         labels.append(encoded_dict['input_ids'])\n",
    "#     return ids, labels, masks\n",
    "    \n",
    "# train_ids, train_labels, train_mask = tokenize(train_input, train_output)\n",
    "# test_ids, test_labels, test_mask = tokenize(test_input, test_output)\n",
    "train_src, train_tar = tokenize(train_input, train_output)\n",
    "test_src, test_tar = tokenize(test_input, test_output)\n",
    "train_src_masks = [[float(i != 0.0) for i in ii] for ii in train_src]\n",
    "train_tar_masks = [[float(i != 0.0) for i in ii] for ii in train_tar]\n",
    "test_src_masks = [[float(i != 0.0) for i in ii] for ii in test_src]\n",
    "test_tar_masks = [[float(i != 0.0) for i in ii] for ii in test_tar]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(train_src, open(\"./data_tmp/train_src.pkl\",'wb'))\n",
    "pickle.dump(train_tar, open(\"./data_tmp/train_tar.pkl\",'wb'))\n",
    "pickle.dump(train_src_masks, open(\"./data_tmp/train_src_masks.pkl\",'wb'))\n",
    "pickle.dump(train_tar_masks, open(\"./data_tmp/train_tar_masks.pkl\",'wb'))\n",
    "\n",
    "\n",
    "pickle.dump(test_src, open(\"./data_tmp/test_src.pkl\",'wb'))\n",
    "pickle.dump(test_tar, open(\"./data_tmp/test_tar.pkl\",'wb'))\n",
    "pickle.dump(test_src_masks, open(\"./data_tmp/test_src_masks.pkl\",'wb'))\n",
    "pickle.dump(test_tar_masks, open(\"./data_tmp/test_tar_masks.pkl\",'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
