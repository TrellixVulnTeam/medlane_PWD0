{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_srcs = './data_tmp/train_srcs.pkl'\n",
    "test_srcs = './data_tmp/test_srcs.pkl'\n",
    "\n",
    "train_txt = './data_tmp/train(12809).txt'\n",
    "test_txt = './data_tmp/test(2030).txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_srcs = pickle.load(open(train_srcs,'rb'))\n",
    "test_srcs = pickle.load(open(test_srcs,'rb'))\n",
    "\n",
    "\n",
    "\n",
    "train_txt = open(train_txt, 'r', encoding = 'utf-8').read().split('\\n\\n')\n",
    "\n",
    "train_tar = []\n",
    "for u in train_txt:\n",
    "    train_tar.append('[CLS] ' + u.split('\\n')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12801\n",
      "2031\n",
      "12801\n",
      "['lastname', 'was', 'admitted', 'to', 'the', 'Neurosurgery', 'service', 'after', 'a', 'study', 'revealed', 'a', 'break', 'in', 'her', '[unused4]', 'catheter', 'at', 'the', 'level', 'of', 'her', 'neck', '.']\n",
      "['Weaned', 'off', 'vent', 'to', 'Continuous positive airway pressure', 'and', 'was', 'extubated', 'in', 'the', 'afternoon', 'on', '9-2', 'by', 'the', 'lung', 'team', '.']\n"
     ]
    }
   ],
   "source": [
    "print(len(train_srcs))\n",
    "print(len(test_srcs))\n",
    "print(len(train_tar))\n",
    "\n",
    "print(train_srcs[4])\n",
    "print(test_srcs[0])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', ')', '[unused4]', ':', 'Patient', 'has', 'history', 'of', 'heavy', 'alcohol', 'abuse', ',', 'and', 'reports', 'having', 'up', 'to', '15-20', 'beers', 'per', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_srcs[11000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[CLS] Stage 2 [MASK] : On sacrum and scrotum .\n",
      "[CLS] Labs were significant for d-dimer 6218 , troponin 0.05 , creatinine 1.4 , bicarbonate 20 and white blood cell 11.3 .\n",
      "[CLS] They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation .\n",
      "[CLS] Serum screens were negative for benzodiazepenes , barbituates and tricyclic antidepressants .\n",
      "[CLS] lastname was admitted to the Neurosurgery service after a study revealed a break in her [MASK] catheter at the level of her neck .\n",
      "[CLS] Her [MASK] had what was likely a resolving [MASK] .\n",
      "[CLS] He was instructed to take plavix for only three months .\n",
      "[CLS] Weaned off vent to Continuous positive airway pressure and was extubated in the afternoon on 9-2 by the lung team .\n",
      "[CLS] She was intubated and was resuscitated after 10-22 minutes of [MASK] arrest .\n",
      "[CLS] He was given succinyl choline and etomidate for intubation .\n",
      "[CLS] Had [MASK] and was transfused with appropriate effect .\n",
      "[CLS] The patient was maintained on logroll precautions until spine films were obtained and read as negative .\n",
      "[CLS] A line placed & sent to [MASK] per neurosurgical recommendations .\n",
      "[CLS] This gradually cleared when the [MASK] was extubated and by the last few days of admission her mental status was at baseline .\n"
     ]
    }
   ],
   "source": [
    "for i,u in enumerate(train_srcs):\n",
    "    tmp = ['[CLS]']\n",
    "    for j,v in enumerate(u):\n",
    "        if v == '[unused4]':\n",
    "#             tmp.append('[SEP]')\n",
    "            tmp.append('[MASK]')\n",
    "#             tmp.append('[SEP]')\n",
    "        else:\n",
    "            tmp.append(v)\n",
    "    train_srcs[i] = tmp\n",
    "    \n",
    "cnt = 0\n",
    "for i,u in enumerate(test_srcs):\n",
    "    tmp = ['[CLS]']\n",
    "    for j,v in enumerate(u):\n",
    "        if v == '[unused4]':\n",
    "            cnt += 1\n",
    "#             tmp.append('[SEP]')\n",
    "            tmp.append('[MASK]')\n",
    "#             tmp.append('[SEP]')\n",
    "        else:\n",
    "            tmp.append(v)\n",
    "    test_srcs[i] = tmp\n",
    "\n",
    "print(cnt)\n",
    "            \n",
    "for i,u in enumerate(train_srcs):\n",
    "    print(' '.join(u))\n",
    "    if i>5:\n",
    "        break\n",
    "for i,u in enumerate(test_srcs):\n",
    "    print(' '.join(u))\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12801\n",
      "12801\n"
     ]
    }
   ],
   "source": [
    "train_src_new = []\n",
    "train_tar_new = []\n",
    "\n",
    "for src, tar in zip(train_srcs, train_tar):\n",
    "    src = ' '.join(src)\n",
    "    src = src.split(' ')\n",
    "    src_new = src\n",
    "    if src_new[-1] !='.':\n",
    "        src_new.append('.')\n",
    "    tar = tar.split(' ')\n",
    "    tar_new = []\n",
    "    p = 0\n",
    "    flag = 0\n",
    "    \n",
    "#     print(' '.join(src_new))\n",
    "#     print(' '.join(tar))\n",
    "    for i,u in enumerate(src):\n",
    "        if u == '[MASK]':\n",
    "            while p<len(tar) and tar[p]!=src[i-1]:\n",
    "                tar_new.append(tar[p])\n",
    "                p+=1\n",
    "            if p<len(tar):\n",
    "                tar_new.append(tar[p])\n",
    "                p+=1\n",
    "            tar_new.append('[SEP]')\n",
    "            while p<len(tar) and tar[p]!=src[i+1]:\n",
    "                tar_new.append(tar[p])\n",
    "                p+=1\n",
    "            tar_new.append('[SEP]')\n",
    "    while p<len(tar):\n",
    "        tar_new.append(tar[p])\n",
    "        p+=1\n",
    "#     print(' '.join(src_new))\n",
    "#     print(' '.join(tar_new))\n",
    "#     print('')\n",
    "    train_src_new.append(src_new)\n",
    "    train_tar_new.append(tar_new)\n",
    "    \n",
    "\n",
    "\n",
    "for i,u in enumerate(train_src_new):\n",
    "    tmp = []\n",
    "    for j,v in enumerate(u):\n",
    "        if v == '[MASK]':\n",
    "            tmp.append('[SEP]')\n",
    "            tmp.append('[MASK]')\n",
    "            tmp.append('[SEP]')\n",
    "        else:\n",
    "            tmp.append(v)\n",
    "    train_src_new[i] = tmp\n",
    "for i,u in enumerate(test_srcs):\n",
    "    tmp = []\n",
    "    for j,v in enumerate(u):\n",
    "        if v == '[MASK]':\n",
    "            tmp.append('[SEP]')\n",
    "            tmp.append('[MASK]')\n",
    "            tmp.append('[SEP]')\n",
    "        else:\n",
    "            tmp.append(v)\n",
    "    test_srcs[i] = tmp\n",
    "print(len(train_src_new))\n",
    "print(len(train_tar_new))\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'She', 'was', 'intubated', 'and', 'was', 'resuscitated', 'after', '10-22', 'minutes', 'of', '[SEP]', '[MASK]', '[SEP]', 'arrest', '.']\n"
     ]
    }
   ],
   "source": [
    "print(test_srcs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_embedding import BertEmbedding\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "bertembedding = BertEmbedding(ctx=mx.gpu(), batch_size=64, max_seq_length=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_tmp/abbrs-all-cased.pkl','rb') as f:\n",
    "    abbrs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ulcers'], [], [], [], ['VPS']]\n",
      "[[], ['PEA'], [], ['anemia'], []]\n"
     ]
    }
   ],
   "source": [
    "train_keys = './data_tmp/train_keys.pkl'\n",
    "test_keys = './data_tmp/test_keys.pkl'\n",
    "\n",
    "train_keys = pickle.load(open(train_keys,'rb'))\n",
    "test_keys = pickle.load(open(test_keys,'rb'))\n",
    "print(train_keys[:5])\n",
    "print(test_keys[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs = []\n",
    "for i,u in enumerate(train_srcs):\n",
    "    p = 0\n",
    "    for v in u:\n",
    "        if v== '[MASK]':\n",
    "            p+=1\n",
    "    if p!=len(train_keys[i]):\n",
    "        print(u, train_keys[i])\n",
    "    if i>30:\n",
    "        break\n",
    "    \n",
    "test_embs = []\n",
    "for i,u in enumerate(test_srcs):\n",
    "    p = 0\n",
    "    for v in u:\n",
    "        if v== '[MASK]':\n",
    "            p+=1\n",
    "    if p!=len(test_keys[i]):\n",
    "        print(u, test_keys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] Stage 2 [SEP] [MASK] [SEP] : On sacrum and scrotum .', '[CLS] Labs were significant for d-dimer 6218 , troponin 0.05 , creatinine 1.4 , bicarbonate 20 and white blood cell 11.3 .', '[CLS] They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation .', '[CLS] Serum screens were negative for benzodiazepenes , barbituates and tricyclic antidepressants .', '[CLS] lastname was admitted to the Neurosurgery service after a study revealed a break in her [SEP] [MASK] [SEP] catheter at the level of her neck .']\n",
      "['[CLS] Stage 2 [SEP] break in skin [SEP] : On sacrum and scrotum .', '[CLS] Labs were significant for d-dimer 6218 , troponin 0.05 , creatinine 1.4 , Bicarbonate 20 and white blood cells 11.3 .', '[CLS] They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation .', '[CLS] Serum toxicology test were negative for benzodiazepenes , barbituates and tricyclic antidepressants .', '[CLS] Patient was admitted to the Neurosurgery service after a study revealed a break in her [SEP] shunt [SEP] catheter at the level of her neck .']\n",
      "['[CLS] Weaned off vent to Continuous positive airway pressure and was extubated in the afternoon on 9-2 by the lung team .', '[CLS] She was intubated and was resuscitated after 10-22 minutes of [SEP] [MASK] [SEP] arrest .', '[CLS] He was given succinyl choline and etomidate for intubation .', '[CLS] Had [SEP] [MASK] [SEP] and was transfused with appropriate effect .', '[CLS] The patient was maintained on logroll precautions until spine films were obtained and read as negative .']\n"
     ]
    }
   ],
   "source": [
    "train_input = []\n",
    "for u in train_src_new:\n",
    "    train_input.append(' '.join(u))\n",
    "\n",
    "test_input = []\n",
    "for u in test_srcs:\n",
    "    test_input.append(' '.join(u))\n",
    "\n",
    "train_output = []\n",
    "for u in train_tar_new:\n",
    "    train_output.append(' '.join(u))\n",
    "\n",
    "print(train_input[:5])\n",
    "print(train_output[:5])\n",
    "print(test_input[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:02<00:00, 4442.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:00<00:00, 4600.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:03<00:00, 3422.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12801\n",
      "2031\n",
      "12801\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from importlib import import_module\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "model_name = 'bert'\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(16)\n",
    "tokenizer = config.tokenizer\n",
    "\n",
    "def tokenize(input_data):\n",
    "    ids = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for i in tqdm(range(len(input_data))):\n",
    "        words = tokenizer.tokenize(input_data[i])\n",
    "        ids.append(words)\n",
    "    \n",
    "    ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in ids],\n",
    "                          maxlen=64, dtype=\"long\", value=0,\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "        \n",
    "    return ids\n",
    " \n",
    "train_src = tokenize(train_input)\n",
    "test_src = tokenize(test_input)\n",
    "train_tar = tokenize(train_output)\n",
    "train_src_masks = [[float(i != 0.0) for i in ii] for ii in train_src]\n",
    "test_src_masks = [[float(i != 0.0) for i in ii] for ii in test_src]\n",
    "train_tar_masks = [[float(i != 0.0) for i in ii] for ii in train_tar]\n",
    "\n",
    "print(len(train_src))\n",
    "print(len(test_src))\n",
    "print(len(train_tar))\n",
    "\n",
    "\n",
    "\n",
    "pickle.dump(train_src, open(\"./data_tmp/train_src_23.pkl\",'wb'))\n",
    "pickle.dump(train_src_masks, open(\"./data_tmp/train_src_masks_23.pkl\",'wb'))\n",
    "\n",
    "\n",
    "pickle.dump(test_src, open(\"./data_tmp/test_src_23.pkl\",'wb'))\n",
    "pickle.dump(test_src_masks, open(\"./data_tmp/test_src_masks_23.pkl\",'wb'))\n",
    "\n",
    "pickle.dump(train_tar, open(\"./data_tmp/train_tar_23.pkl\",'wb'))\n",
    "pickle.dump(train_tar_masks, open(\"./data_tmp/train_tar_masks_23.pkl\",'wb'))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12801it [1:22:56,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train_embs = []\n",
    "for (srcs, keys) in tqdm(zip(train_srcs,train_keys)):\n",
    "    key_embs = []\n",
    "    for key in keys:\n",
    "        emb_values = []\n",
    "        for i, value in enumerate(abbrs[key]):\n",
    "            if i>=14:\n",
    "                break\n",
    "            embs = bertembedding.embedding([value])\n",
    "            vec = np.mean(embs[0][1], axis=0)\n",
    "            emb_values.append(vec)\n",
    "        while len(emb_values)<14:\n",
    "            embs = bertembedding.embedding(['[PAD] [PAD]'])\n",
    "            vec = np.mean(embs[0][1], axis=0)\n",
    "            emb_values.append(vec)\n",
    "            \n",
    "        key_embs.append(emb_values)\n",
    "    train_embs.append(key_embs)\n",
    "#     if len(train_embs)>=250:\n",
    "#         break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12801\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_embs, open('./data_tmp/train_embs.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2031it [11:14,  3.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "test_embs = []\n",
    "for (srcs, keys) in tqdm(zip(test_srcs,test_keys)):\n",
    "    key_embs = []\n",
    "    for key in keys:\n",
    "        emb_values = []\n",
    "        for i, value in enumerate(abbrs[key]):\n",
    "            if i>=14:\n",
    "                break\n",
    "            embs = bertembedding.embedding([value])\n",
    "            vec = np.mean(embs[0][1], axis=0)\n",
    "            emb_values.append(vec)\n",
    "        while len(emb_values)<14:\n",
    "            embs = bertembedding.embedding(['[PAD] [PAD]'])\n",
    "            vec = np.mean(embs[0][1], axis=0)\n",
    "            emb_values.append(vec)\n",
    "            \n",
    "        key_embs.append(emb_values)\n",
    "    test_embs.append(key_embs)\n",
    "#     if len(train_embs)>=250:\n",
    "#         break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test_embs, open('./data_tmp/test_embs.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1 = './data_tmp/test_preds123.pkl'\n",
    "scores_2 = './data_tmp/test_preds.pkl'\n",
    "\n",
    "scores_1 = pickle.load(open(scores_1,'rb'))\n",
    "scores_2 = pickle.load(open(scores_2,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.9477182e-03 1.2834789e-04 8.4618572e-05 3.7187710e-04 9.9746525e-01\n",
      " 2.0347509e-06 1.1513179e-09 1.8726982e-09 3.0972069e-09 7.6202554e-08\n",
      " 4.1563894e-08 1.3514008e-09 1.3164096e-09 3.0789302e-09]\n",
      "[3.2233633e-03 2.5538218e-04 1.3327619e-05 9.7694927e-01 1.3771916e-07\n",
      " 1.9558027e-02 3.9938206e-07 2.2134707e-08 1.3474428e-08 8.2160092e-09\n",
      " 1.1622502e-09 1.0599923e-09 1.7210798e-08 5.0620068e-09]\n",
      "[9.9295852e-05 9.9963236e-01 2.6836587e-04 3.1079156e-10 2.7242822e-12\n",
      " 2.9964149e-11 8.2205531e-13 2.7669403e-12 2.4095935e-11 1.0100639e-12\n",
      " 5.7707268e-12 1.9868390e-11 2.2406469e-12 7.3201384e-12]\n",
      "[1.1550642e-03 9.9884486e-01 6.9386141e-10 2.0959284e-10 7.6499571e-12\n",
      " 1.8509590e-11 2.2780376e-12 1.9910597e-12 3.7005638e-11 1.7116905e-11\n",
      " 3.0358296e-12 3.8686697e-12 2.3394954e-12 1.6520859e-12]\n",
      "[1.7860773e-05 9.9910921e-01 4.4506535e-04 4.2785192e-04 1.9317308e-11\n",
      " 4.0426148e-12 1.4503823e-12 2.7345927e-12 7.3280840e-11 6.9505740e-13\n",
      " 1.8358497e-11 7.1967540e-12 2.8996945e-11 3.0508914e-12]\n",
      "[9.7007287e-01 5.1083980e-04 1.9883996e-04 2.2970153e-02 5.2719163e-03\n",
      " 9.7189005e-04 1.3442012e-06 1.7698164e-10 2.4108784e-08 3.5355857e-10\n",
      " 2.7308013e-07 1.0306275e-09 1.0204730e-06 9.6187557e-07]\n",
      "[9.9975497e-01 6.4379710e-05 1.8066105e-04 4.7552240e-10 2.8425992e-11\n",
      " 6.8868756e-12 3.9083917e-11 9.4978990e-12 2.7009560e-11 1.1305181e-11\n",
      " 6.4472013e-12 2.0860426e-11 6.9037228e-12 4.9577538e-12]\n",
      "13467\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    if len(x.shape) > 1:\n",
    "        # 矩阵\n",
    "        tmp = np.max(x,axis=1) # 得到每行的最大值，用于缩放每行的元素，避免溢出\n",
    "        x -= tmp.reshape((x.shape[0],1)) # 利用性质缩放元素\n",
    "        x = np.exp(x) # 计算所有值的指数\n",
    "        tmp = np.sum(x, axis = 1) # 每行求和        \n",
    "        x /= tmp.reshape((x.shape[0], 1)) # 求softmax\n",
    "    else:\n",
    "        # 向量\n",
    "        tmp = np.max(x) # 得到最大值\n",
    "        x -= tmp # 利用最大值缩放数据\n",
    "        x = np.exp(x) # 对所有元素求指数        \n",
    "        tmp = np.sum(x) # 求元素和\n",
    "        x /= tmp # 求somftmax\n",
    "    return x\n",
    "\n",
    "for i,u in enumerate(scores_1):\n",
    "    print(u.asnumpy())\n",
    "    if i>5:\n",
    "        break\n",
    "print(len(scores_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.7378085e-01 1.2614059e-01 6.0028018e-05 1.8474586e-05 3.4883456e-08\n",
      " 2.5952373e-08 1.6697186e-08 8.8717167e-09 8.1225169e-09 3.4625642e-09\n",
      " 1.9747584e-09 4.2847116e-08 1.1821275e-09 4.4037147e-08]\n",
      "[5.1649287e-03 9.6286654e-01 3.4913076e-03 2.8167555e-02 3.0967293e-04\n",
      " 4.8246065e-09 2.1307014e-09 4.6686472e-09 4.8952207e-09 1.4119332e-09\n",
      " 7.0544670e-10 5.1246385e-10 5.6442007e-11 2.0266472e-10]\n",
      "[9.90064722e-03 9.90097761e-01 1.67028384e-06 4.05945499e-09\n",
      " 3.19267857e-09 1.64763939e-10 7.58567764e-10 9.49660883e-09\n",
      " 1.07584094e-10 8.38450837e-11 2.38044667e-10 1.35902765e-11\n",
      " 7.27654673e-11 1.07435623e-10]\n",
      "[1.0568612e-03 5.5159533e-01 4.4267687e-01 1.5153284e-03 2.8644700e-03\n",
      " 2.1330939e-04 7.7836528e-05 6.3519917e-10 7.9424254e-09 5.1189799e-11\n",
      " 1.4415227e-10 1.8019497e-10 1.2605858e-11 4.5535752e-11]\n",
      "[9.7215682e-02 8.7176198e-01 1.0939831e-02 2.0061418e-02 1.2920410e-06\n",
      " 1.6896263e-05 1.1870497e-08 2.2082249e-08 8.6618158e-08 2.2316017e-06\n",
      " 3.8285506e-07 1.1652103e-07 5.4344735e-08 8.1841183e-09]\n",
      "[1.5086643e-03 9.7159123e-01 1.9595655e-02 6.8771106e-04 6.5947766e-04\n",
      " 6.7190389e-04 5.2853171e-03 4.0493671e-09 1.3767216e-09 1.3752441e-09\n",
      " 6.3242378e-10 3.1356728e-10 1.3004194e-09 3.1950737e-10]\n",
      "[2.87869188e-04 9.96429503e-01 1.06327259e-03 2.20845942e-03\n",
      " 1.09392786e-05 6.31978081e-11 1.72621112e-10 2.71762363e-10\n",
      " 3.58790608e-10 2.53417148e-10 5.39138630e-11 1.22367195e-11\n",
      " 3.15185183e-12 5.50372248e-11]\n",
      "13442\n"
     ]
    }
   ],
   "source": [
    "for i,u in enumerate(scores_2):\n",
    "    print(u.asnumpy())\n",
    "    if i>5:\n",
    "        break\n",
    "print(len(scores_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
