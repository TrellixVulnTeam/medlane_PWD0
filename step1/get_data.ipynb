{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==2.6.0\n",
    "!pip install keras==2.1.6\n",
    "!pip install numpy==1.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5py==2.8.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.7.1+cu110'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "bs = 16 # batch_size\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')\n",
    "\n",
    "def word_tokenize(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    temp = ''\n",
    "    for word in tokens:\n",
    "        if word[0] == '#':\n",
    "            word = word[2:]\n",
    "            temp += word\n",
    "        else:\n",
    "            temp += ' '\n",
    "            temp += word\n",
    "    temp = temp.strip()\n",
    "    \n",
    "    return temp.split(' ')\n",
    "        \n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "dic_list = []\n",
    "train_dic = {}\n",
    "mask_counter = 0\n",
    "with open('../data_tmp/abbrs-all.pkl','rb') as f:\n",
    "    abbrs = pickle.load(f)\n",
    "\n",
    "def get_mask(src_words, tar_words):\n",
    "    if tar_words[-1] != '.':\n",
    "        tar_words.append('.')\n",
    "    i = 0\n",
    "    j = 0\n",
    "    mask = []\n",
    "\n",
    "    while i < len(src_words):\n",
    "        if j==len(tar_words):\n",
    "            while i<len(src_words):\n",
    "                mask.append(0)\n",
    "                i+=1\n",
    "            break\n",
    "            \n",
    "        if src_words[i] == tar_words[j]:\n",
    "            mask.append(0)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            p = i + 1\n",
    "            q = j + 1\n",
    "\n",
    "            while p < len(src_words):\n",
    "                while q < len(tar_words) and tar_words[q] != src_words[p]:\n",
    "                    q += 1\n",
    "                if q == len(tar_words):\n",
    "                    p = p + 1\n",
    "                    q = j + 1\n",
    "                else:\n",
    "                    break\n",
    "            aft = \" \".join(tar_words[j:q])\n",
    "            for k, word in enumerate(src_words[i:p]):\n",
    "                key = word\n",
    "                if key in abbrs.keys():\n",
    "                    pass\n",
    "                elif key.upper() in abbrs.keys():\n",
    "                    key = key.upper()\n",
    "                elif key.lower() in abbrs.keys():\n",
    "                    key = key.lower()\n",
    "                    \n",
    "                if key in abbrs.keys():\n",
    "                    mask.append(1)\n",
    "                else:\n",
    "                    mask.append(0)\n",
    "                    \n",
    "            i = p\n",
    "            j = q\n",
    "\n",
    "    return mask\n",
    "\n",
    "def get_train_data():\n",
    "    src = []\n",
    "    tar = []\n",
    "    txt = ''\n",
    "    try:\n",
    "        txt += open('../data/train(12809).txt', 'r').read()\n",
    "    except:\n",
    "        txt += open('../data/train(12809).txt', 'r', encoding='utf-8').read()\n",
    "  \n",
    "    txt = txt.split('\\n\\n')\n",
    "    mask_new = []\n",
    "    src_new = []\n",
    "    for para in tqdm(txt):\n",
    "        sentences = para.split('\\n')\n",
    "        if len(sentences) < 2:\n",
    "            continue\n",
    "        for sid, sentence in enumerate(sentences[0:2]):\n",
    "            if sid == 0:\n",
    "                src.append(sentence)\n",
    "            else:\n",
    "                tar.append(sentence)\n",
    "\n",
    "    for i in range(len(src)):\n",
    "        src_sentence = src[i]\n",
    "        tar_sentence = tar[i]\n",
    "        src_words = word_tokenize(src_sentence)\n",
    "        tar_words = word_tokenize(tar_sentence)\n",
    "        mask = get_mask(src_words, tar_words)\n",
    "        src_new.append(src_words)\n",
    "        mask_new.append(mask)\n",
    "        assert len(src_words) == len(mask)\n",
    "        \n",
    "    return src_new, mask_new\n",
    "\n",
    "def get_test_data():\n",
    "    txt = open('../data/test(2030).txt', 'r').read()\n",
    "#     txt = txt.lower()\n",
    "    txt = txt.split('\\n\\n')\n",
    "    mask = []\n",
    "    src_new = []\n",
    "    for para in tqdm(txt):\n",
    "        sentences = para.split('\\n')\n",
    "        masks = []\n",
    "        \n",
    "        src_sentence = ''\n",
    "        \n",
    "        if len(sentences) < 2 or len(sentences[0]) < 3 or len(sentences[1]) < 3:\n",
    "            continue\n",
    "        for sid, sentence in enumerate(sentences):\n",
    "            if sid == 0:\n",
    "                src_sentence = sentence\n",
    "                words = word_tokenize(sentence)\n",
    "                src_new.append(words)\n",
    "                \n",
    "            elif sid == 1:\n",
    "                cudic = {}\n",
    "                sentence = sentence[2:].lower()\n",
    "                text = re.sub('\\[[^\\[\\]]*\\]', '', sentence)\n",
    "                pairs = re.findall('[^\\[\\] ]+\\[[^\\[\\]]+\\]', sentence)\n",
    "                for pair in pairs:\n",
    "                    pair = re.split('[\\[\\]]', pair)\n",
    "                    cudic[pair[0]] = pair[1]\n",
    "                    dic_list.append(pair)\n",
    "                words = word_tokenize(text)\n",
    "                for wid, word in enumerate(words):\n",
    "                    if word in cudic.keys():\n",
    "                        words[wid] = cudic[word]\n",
    "                new_text = ''\n",
    "                for word in words:\n",
    "                    new_text += word\n",
    "                    new_text += ' '\n",
    "                masks=get_mask(word_tokenize(src_sentence), word_tokenize(new_text))\n",
    "            \n",
    "        mask.append(masks)\n",
    "    return src_new, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['late stage of scarring', 'hard liver disease', 'chronic disease of the liver', 'cirrhosis（a condition in which the liver does not function properly due to long-term damage）']\n",
      "['cpap']\n"
     ]
    }
   ],
   "source": [
    "print(abbrs['cirrhosis'])\n",
    "print(tokenizer.tokenize('CPAP'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:00<00:00, 782056.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:03<00:00, 620.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels = get_train_data()\n",
    "test_sentences, test_labels = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['stage', '2', 'ulcers', ':', 'on', 'sacrum', 'and', 'scrotum', '.'], ['labs', 'were', 'significant', 'for', 'd', '-', 'dimer', '6218', ',', 'troponin', '0', '.', '05', ',', 'creatinine', '1', '.', '4', ',', 'hco3', '20', 'and', 'wbc', '11', '.', '3', '.'], ['they', 'drove', 'down', 'from', 'country', '6607', 'to', 'hospital1', '18', 'emergency', 'department', 'for', 'evaluation', '.'], ['serum', 'tox', 'screens', 'were', 'negative', 'for', 'benzodiazepenes', ',', 'barbituates', 'and', 'tricyclic', 'antidepressants', '.'], ['known', 'lastname', 'was', 'admitted', 'to', 'the', 'neurosurgery', 'service', 'after', 'a', 'study', 'revealed', 'a', 'break', 'in', 'her', 'vps', 'catheter', 'at', 'the', 'level', 'of', 'her', 'neck', '.']]\n",
      "[[0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[['weaned', 'off', 'vent', 'to', 'cpap', 'and', 'was', 'extubated', 'in', 'the', 'afternoon', 'on', '9', '-', '2', 'by', 'the', 'pulmonary', 'team', '.'], ['she', 'was', 'intubated', 'and', 'was', 'resuscitated', 'after', '10', '-', '22', 'minutes', 'of', 'pea', 'arrest', '.'], ['he', 'was', 'given', 'succinyl', 'choline', 'and', 'etomidate', 'for', 'intubation', '.'], ['had', 'post', '-', 'op', 'anemia', 'and', 'was', 'transfused', 'with', 'appropriate', 'effect', '.'], ['the', 'patient', 'was', 'maintained', 'on', 'logroll', 'precautions', 'until', 'tl', 'spine', 'films', 'were', 'obtained', 'and', 'read', 'as', 'negative', '.']]\n",
      "[[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[:5])\n",
    "print(train_labels[:5])\n",
    "print(test_sentences[:5])\n",
    "print(test_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_values = [0, 1]\n",
    "tag_values.append(2)\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "\n",
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(train_sentences, train_labels)\n",
    "]\n",
    "\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[2], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "pickle.dump(input_ids, open(\"./data/input_ids\",'wb'))\n",
    "pickle.dump(tags, open(\"./data/tags\",'wb'))\n",
    "pickle.dump(attention_masks, open(\"./data/attention_masks\",'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenized_texts_and_labels = [\n",
    "    tokenize_and_preserve_labels(sent, labs)\n",
    "    for sent, labs in zip(test_sentences, test_labels)\n",
    "]\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
    "                     maxlen=MAX_LEN, value=tag2idx[2], padding=\"post\",\n",
    "                     dtype=\"long\", truncating=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "pickle.dump(input_ids, open(\"./data/test_input_ids\",'wb'))\n",
    "pickle.dump(tags, open(\"./data/test_tags\",'wb'))\n",
    "pickle.dump(attention_masks, open(\"./data/test_attention_masks\",'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
