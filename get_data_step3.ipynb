{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import pickle\n",
    "import re\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:00<00:00, 832500.47it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = open('./data_tmp/train(12809).txt', 'r', encoding='utf-8').read()\n",
    "txt = txt.split('\\n\\n')\n",
    "train_input = []\n",
    "train_output = []\n",
    "for para in tqdm(txt):\n",
    "    sentences = para.split('\\n')\n",
    "    st1 = sentences[1]\n",
    "    st2 = sentences[2]\n",
    "    train_input.append(st1)\n",
    "    train_output.append(st2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentence(sentence, cudic):\n",
    "    sentence = sentence[2:]\n",
    "    text = re.sub('\\[[^\\[\\]]*\\]', '', sentence)\n",
    "    pairs = re.findall('[^\\[\\] ]+\\[[^\\[\\]]+\\]', sentence)\n",
    "    for pair in pairs:\n",
    "        pair = re.split('[\\[\\]]', pair)\n",
    "        cudic[pair[0]] = pair[1]\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for wid, word in enumerate(words):\n",
    "        if word in cudic.keys():\n",
    "            words[wid] = cudic[word]\n",
    "    return ' '.join(words), cudic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:00<00:00, 3398.14it/s]\n"
     ]
    }
   ],
   "source": [
    "txt = open('./data_tmp/test(2030).txt', 'r', encoding='utf-8').read()\n",
    "txt = txt.split('\\n\\n')\n",
    "test_input = []\n",
    "test_output = []\n",
    "cudics = []\n",
    "for para in tqdm(txt):\n",
    "    sentences = para.split('\\n')\n",
    "    st1 = sentences[1]\n",
    "    st2 = sentences[2]\n",
    "    cudic = {}\n",
    "    st_, cu = to_sentence(st1,cudic)\n",
    "    cudic.update(cu)\n",
    "    test_input.append(st_)\n",
    "    st_, cu = to_sentence(st2,cudic)\n",
    "    cudic.update(cu)\n",
    "    test_output.append(st_)\n",
    "    cudics.append(cudic)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(cudics,open('./data_tmp/test_cudics.pkl','wb'))\n",
    "pickle.dump(test_output,open('./data_tmp/test_tars.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Stage 2 break in skin : On sacrum and scrotum .', 'Labs were significant for d-dimer 6218 , troponin 0.05 , creatinine 1.4 , Bicarbonate 20 and white blood cells 11.3 .', 'They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation .', 'Serum toxicology test were negative for benzodiazepenes , barbituates and tricyclic antidepressants .', 'Patient was admitted to the Neurosurgery service after a study revealed a break in her shunt catheter at the level of her neck .']\n",
      "['Stage 2 break in skin : On sacrum and scrotum .', 'Labs were significant for d-dimer 6218 , troponin 0.05 , creatinine 1.4 , Bicarbonate 20 and white blood cells 11.3 .', 'They drove down from Country 6607 to Hospital1 18 Emergency Department for evaluation .', 'Serum toxicology test were negative for benzodiazepenes , barbituates and tricyclic antidepressants .', 'Patient was admitted to the Neurosurgery service after a study revealed a break in your shunt catheter at the level of your neck .']\n",
      "['Weaned off vent to continuous positive airway pressure and was extubated in the afternoon on 9-2 by the lung specialist team .', 'She was intubated and was waken up after 10-22 minutes of pulseless electrical activity arrest .', 'He was given succinyl choline and etomidate for intubation .', 'Had post-op Lack of enough healthy red blood cells and was transfused with appropriate effect .', 'The patient was maintained on logroll diseases prevention until TL spine films were obtained and read as negative .']\n",
      "['Weaned off vent to continuous positive airway pressure and was extubated in the afternoon on 9-2 by the lung specialist team .', 'She was intubated and was waken up after 10-22 minutes of unresponsive pulse arrest .', 'He was given succinyl choline and etomidate for intubation .', 'Had post-op Lack of enough healthy red blood cells and was transfused with appropriate effect .', 'The patient was maintained on logroll diseases prevention until TL spine films were obtained and read as negative .']\n",
      "[{'CPAP': 'continuous positive airway pressure'}, {'PEA': 'pulseless electrical activity'}, {}, {}, {}]\n"
     ]
    }
   ],
   "source": [
    "print(train_input[:5])\n",
    "print(train_output[:5])\n",
    "print(test_input[:5])\n",
    "print(test_output[:5])\n",
    "print(cudics[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(src_words, tar_words):\n",
    "    temp = []\n",
    "    tar_ptr = 0\n",
    "    for i, word in enumerate(src_words):\n",
    "        if i+1 < len(src_words) and src_words[i+1] in tar_words[tar_ptr:]:\n",
    "            j = tar_words.index(src_words[i+1], tar_ptr)+1\n",
    "        else:\n",
    "            j = len(tar_words)\n",
    "        if word not in tar_words[tar_ptr:j]:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(0)\n",
    "            tar_ptr = tar_words.index(word, tar_ptr) + 1\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:03<00:00, 3258.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_input))):\n",
    "    st1 = train_input[i]\n",
    "    st2 = train_output[i]\n",
    "    words1 = nltk.word_tokenize(st1)\n",
    "    words2 = nltk.word_tokenize(st2)\n",
    "    masks=get_mask(words1, words2)\n",
    "    new_words1 = []\n",
    "    new_words2 = []\n",
    "    new_words1.append('[CLS]')\n",
    "    new_words2.append('[CLS]')\n",
    "    new_words2.extend(words2)\n",
    "    flag = 0\n",
    "    for j, mask in enumerate(masks):\n",
    "        if mask == 0 and flag == 1:\n",
    "            new_words1.append('[unused2]')\n",
    "            flag = 0\n",
    "        elif mask == 1 and flag == 0:\n",
    "            new_words1.append('[unused2]')\n",
    "            flag = 1\n",
    "        new_words1.append(words1[j])\n",
    "    new_words1.append('[SEP]')\n",
    "    new_words2.append('[SEP]')\n",
    "    train_input[i] = ' '.join(new_words1)\n",
    "    train_output[i] = ' '.join(new_words2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:00<00:00, 3551.39it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(test_input))):\n",
    "    st1 = test_input[i]\n",
    "    st2 = test_output[i]\n",
    "    words1 = nltk.word_tokenize(st1)\n",
    "    words2 = nltk.word_tokenize(st2)\n",
    "    masks=get_mask(words1, words2)\n",
    "    new_words1 = []\n",
    "    new_words2 = []\n",
    "    new_words1.append('[CLS]')\n",
    "    new_words2.append('[CLS]')\n",
    "    new_words2.extend(words2)\n",
    "    flag = 0\n",
    "    for j, mask in enumerate(masks):\n",
    "        if mask == 0 and flag == 1:\n",
    "            new_words1.append('[unused2]')\n",
    "            flag = 0\n",
    "        elif mask == 1 and flag == 0:\n",
    "            new_words1.append('[unused2]')\n",
    "            flag = 1\n",
    "        new_words1.append(words1[j])\n",
    "    new_words1.append('[SEP]')\n",
    "    new_words2.append('[SEP]')\n",
    "    test_input[i] = ' '.join(new_words1)\n",
    "    test_output[i] = ' '.join(new_words2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] patient is a 89 year old male with [unused2] sick sinus syndrome status post [unused2] pacer , admitted to Neurosurgery with [unused2] subdural hematoma [unused2] with acute respiratory distress/ wheezing on the floor . [SEP]\n",
      "[CLS] patient is a 89 year old male with irregular heart rhythms after pacer , admitted to Neurosurgery with brain blood collects with acute respiratory distress/ wheezing on the floor . [SEP]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,(u,v) in enumerate(zip(test_input,test_output)):\n",
    "    if 'patient is a 89 year old' in u:\n",
    "        print(u)\n",
    "        print(v)\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_tmp/train_input.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_input,f)\n",
    "with open(\"./data_tmp/train_output.pkl\", 'wb') as f:\n",
    "    pickle.dump(train_output,f)\n",
    "with open(\"./data_tmp/test_input.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_input,f)\n",
    "with open(\"./data_tmp/test_output.pkl\", 'wb') as f:\n",
    "    pickle.dump(test_output,f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:06<00:00, 1979.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:00<00:00, 2097.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from importlib import import_module\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "model_name = 'bert'\n",
    "x = import_module('models.' + model_name)\n",
    "config = x.Config(16)\n",
    "tokenizer = config.tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = False)\n",
    "\n",
    "def tokenize(input_data, output_data):\n",
    "    ids = []\n",
    "    labels = []\n",
    "    masks = []\n",
    "    for i in tqdm(range(len(input_data))):\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                                 input_data[i],                      # Sentence to encode.\n",
    "#                                 add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
    "#                                 max_length = 64,           # Pad & truncate all sentences.\n",
    "#                                 pad_to_max_length = True,\n",
    "#                                 return_attention_mask = True,   # Construct attn. masks.\n",
    "#                                 return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                            )\n",
    "#         words = nltk.word_tokenize(input_data[i])\n",
    "#         tokenized_sentence = []\n",
    "#         for word in words:\n",
    "#             tokenized_word = tokenizer.tokenize(word)\n",
    "#             tokenized_sentence.extend(tokenized_word)\n",
    "#         ids.append(tokenized_sentence)\n",
    "        \n",
    "#         words = nltk.word_tokenize(output_data[i])\n",
    "#         tokenized_sentence = []\n",
    "#         for word in words:\n",
    "#             tokenized_word = tokenizer.tokenize(word)\n",
    "#             tokenized_sentence.extend(tokenized_word)\n",
    "#         labels.append(tokenized_sentence)\n",
    "        words = tokenizer.tokenize(input_data[i])\n",
    "        ids.append(words)\n",
    "        words = tokenizer.tokenize(output_data[i])\n",
    "        labels.append(words)\n",
    "        \n",
    "        \n",
    "    \n",
    "    ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in ids],\n",
    "                          maxlen=64, dtype=\"long\", value=0,\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    labels = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in labels],\n",
    "                          maxlen=64, dtype=\"long\", value=0,\n",
    "                              truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "        \n",
    "    return ids, labels\n",
    "    \n",
    "#         ids.append(encoded_dict['input_ids'])\n",
    "#         masks.append(encoded_dict['attention_mask'])\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                                 output_data[i],add_special_tokens = False, max_length = 64, pad_to_max_length = True, return_attention_mask = True,\n",
    "#                                 return_tensors = 'pt',)\n",
    "#         labels.append(encoded_dict['input_ids'])\n",
    "#     return ids, labels, masks\n",
    "    \n",
    "# train_ids, train_labels, train_mask = tokenize(train_input, train_output)\n",
    "# test_ids, test_labels, test_mask = tokenize(test_input, test_output)\n",
    "train_src, train_tar = tokenize(train_input, train_output)\n",
    "test_src, test_tar= tokenize(test_input, test_output)\n",
    "train_src_masks = [[float(i != 0.0) for i in ii] for ii in train_src]\n",
    "train_tar_masks = [[float(i != 0.0) for i in ii] for ii in train_tar]\n",
    "test_src_masks = [[float(i != 0.0) for i in ii] for ii in test_src]\n",
    "test_tar_masks = [[float(i != 0.0) for i in ii] for ii in test_tar]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(train_src, open(\"./data_tmp/train_src.pkl\",'wb'))\n",
    "pickle.dump(train_tar, open(\"./data_tmp/train_tar.pkl\",'wb'))\n",
    "pickle.dump(train_src_masks, open(\"./data_tmp/train_src_masks.pkl\",'wb'))\n",
    "pickle.dump(train_tar_masks, open(\"./data_tmp/train_tar_masks.pkl\",'wb'))\n",
    "\n",
    "\n",
    "pickle.dump(test_src, open(\"./data_tmp/test_src.pkl\",'wb'))\n",
    "pickle.dump(test_tar, open(\"./data_tmp/test_tar.pkl\",'wb'))\n",
    "pickle.dump(test_src_masks, open(\"./data_tmp/test_src_masks.pkl\",'wb'))\n",
    "pickle.dump(test_tar_masks, open(\"./data_tmp/test_tar_masks.pkl\",'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30519 0\n",
      "30511 0\n",
      "30519 0\n",
      "30511 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.max(train_src),np.min(train_src))\n",
    "print(np.max(test_src),np.min(test_src))\n",
    "print(np.max(train_tar),np.min(train_tar))\n",
    "print(np.max(test_tar),np.min(test_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ids = torch.tensor(train_ids).to(torch.int64)\n",
    "test_ids = torch.tensor(test_ids).to(torch.int64)\n",
    "train_masks = torch.tensor(train_masks).to(torch.int64)\n",
    "test_masks = torch.tensor(test_masks).to(torch.int64)\n",
    "train_labels = torch.tensor(train_labels).to(torch.int64)\n",
    "test_labels = torch.tensor(test_labels).to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# st = set()\n",
    "# for u in train_labels:\n",
    "#     L = np.array(u)[0]\n",
    "#     for v in L:\n",
    "#         st.add(v)\n",
    "# print(len(st))\n",
    "# for u in test_labels:\n",
    "#     L = np.array(u)[0]\n",
    "#     for v in L:\n",
    "#         st.add(v)\n",
    "# print(len(st))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids = torch.cat(train_ids, dim=0)\n",
    "# train_mask = torch.cat(train_mask, dim=0)\n",
    "# train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "# test_ids = torch.cat(test_ids, dim=0)\n",
    "# test_mask = torch.cat(test_mask, dim=0)\n",
    "# test_labels = torch.cat(test_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels.max())\n",
    "print(train_labels.min())\n",
    "print(train_ids.max())\n",
    "print(train_ids.min())\n",
    "\n",
    "print(test_labels.max())\n",
    "print(test_labels.min())\n",
    "print(test_ids.max())\n",
    "print(test_ids.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(torch.__version__)\n",
    "\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
    "\n",
    "valid_data = TensorDataset(test_ids, test_masks, test_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\",num_labels=3,\n",
    "                                                    output_attentions = False,\n",
    "                                                    output_hidden_states = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=5e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "epochs = 3\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(n_gpu)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_values, validation_loss_values = [], []\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, token_type_ids=None,\n",
    "                        attention_mask=b_input_mask, labels=b_labels)\n",
    "        # get the loss\n",
    "        loss = outputs[0]\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()\n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "#         b_input_ids = torch.tensor(b_input_ids).to(torch.int64)\n",
    "#         b_input_mask = torch.tensor(b_input_mask).to(torch.int64)\n",
    "#         b_labels = torch.tensor(b_labels).to(torch.int64)\n",
    "\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            outputs = model(b_input_ids, token_type_ids=None,\n",
    "                            attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        eval_loss += outputs[0].mean().item()\n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "\n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PADT\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PADT\"]\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
