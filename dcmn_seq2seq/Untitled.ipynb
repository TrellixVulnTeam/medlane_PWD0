{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel,BertModel,BertConfig,BertPooler\n",
    "import torch\n",
    "from models.bert import Config\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def merge_mask(src, mask, tokenizer):\n",
    "    mask_new = []\n",
    "    i = 0\n",
    "    for word in src:\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        if n_subwords > 1:\n",
    "            if 1 in mask[i:i + n_subwords]:\n",
    "                mask_new.append(1)\n",
    "            else:\n",
    "                mask_new.append(0)\n",
    "        else:\n",
    "            mask_new.append(mask[i])\n",
    "        i += n_subwords\n",
    "\n",
    "    return mask_new\n",
    "\n",
    "\n",
    "def simplify(word):\n",
    "    new_word = word.lower().replace('( ', '(').replace(' )', ')').replace('  ', ' ').replace(' ,', ',').strip()\n",
    "    return new_word\n",
    "\n",
    "\n",
    "def min_distance(str1, str2):\n",
    "    matrix = [[i + j for j in range(len(str2) + 1)] for i in range(len(str1) + 1)]\n",
    "\n",
    "    for i in range(1, len(str1) + 1):\n",
    "        for j in range(1, len(str2) + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "            matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + d)\n",
    "\n",
    "    return matrix[len(str1)][len(str2)]\n",
    "\n",
    "\n",
    "def is_similar(word_1, word_2, stop=False):\n",
    "    word_1 = simplify(word_1)\n",
    "    word_2 = simplify(word_2)\n",
    "    flag = False\n",
    "    if ('(' in word_1 or '(' in word_2) and not stop:\n",
    "        _word_1 = re.sub(u\"\\\\(.*?\\\\)|\\\\{.*?}|\\\\[.*?]\", \"\", word_1)\n",
    "        _word_2 = re.sub(u\"\\\\(.*?\\\\)|\\\\{.*?}|\\\\[.*?]\", \"\", word_2)\n",
    "        flag = is_similar(_word_1, _word_2, True)\n",
    "\n",
    "    return word_1 == word_2 or \\\n",
    "           word_1 in word_2 or \\\n",
    "           word_2 in word_1 or \\\n",
    "           min_distance(word_1, word_2) <= 2 or \\\n",
    "           flag\n",
    "\n",
    "def get_train_src_tar_txt(train_txt_path):\n",
    "    src = []\n",
    "    tar_1 = []\n",
    "    tar_2 = []\n",
    "    txt = ''\n",
    "    try:\n",
    "        txt += open(train_txt_path, 'r').read()\n",
    "    except:\n",
    "        txt += open(train_txt_path, 'r', encoding='utf-8').read()\n",
    "\n",
    "    txt = txt.split('\\n\\n')\n",
    "    for para in txt:\n",
    "        sentences = para.split('\\n')\n",
    "        if len(sentences) < 2:\n",
    "            continue\n",
    "        for sid, sentence in enumerate(sentences[0:3]):\n",
    "            if sid == 0:\n",
    "                src.append(sentence)\n",
    "            elif sid == 1:\n",
    "                tar_1.append(sentence)\n",
    "            elif sid == 2:\n",
    "                tar_2.append(sentence)\n",
    "    return src, tar_1, tar_2\n",
    "\n",
    "def get_test_src_tar_txt(test_txt_path):\n",
    "    txt = open(test_txt_path, 'r').read()\n",
    "    #     txt = txt.lower()\n",
    "    txt = txt.split('\\n\\n')\n",
    "    src = []\n",
    "    tar_1 = []\n",
    "    tar_2 = []\n",
    "    for para in txt:\n",
    "        sentences = para.split('\\n')\n",
    "        src_sentence = ''\n",
    "        if len(sentences) < 2 or len(sentences[0]) < 3 or len(sentences[1]) < 3:\n",
    "            continue\n",
    "        for sid, sentence in enumerate(sentences):\n",
    "            if sid == 0:\n",
    "                src.append(sentence)\n",
    "            elif sid <=2:\n",
    "                cudic = {}\n",
    "                sentence = sentence[2:]\n",
    "                sentence = sentence.replace('].', '] .')\n",
    "                text = re.sub('\\[[^\\[\\]]*\\]', '', sentence)\n",
    "                pairs = re.findall('[^\\[\\] ]+\\[[^\\[\\]]+\\]', sentence)\n",
    "                for pair in pairs:\n",
    "                    pair = re.split('[\\[\\]]', pair)\n",
    "                    cudic[pair[0]] = pair[1]\n",
    "                words = nltk.word_tokenize(text)\n",
    "                for wid, word in enumerate(words):\n",
    "                    if word in cudic.keys():\n",
    "                        words[wid] = cudic[word]\n",
    "                new_text = ' '.join(words)\n",
    "                if sid == 1:\n",
    "                    tar_1.append(new_text)\n",
    "                else:\n",
    "                    tar_2.append(new_text)\n",
    "    return src, tar_1, tar_2\n",
    "\n",
    "def get_dcmn_data_from_gt(src_words, tar_words, abbrs, max_pad_length, max_dcmn_seq_length, tokenizer):\n",
    "    if tar_words[-1] != '.':\n",
    "        tar_words.append('.')\n",
    "    i = 0\n",
    "    j = 0\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    src = ['[CLS]']\n",
    "    keys = []\n",
    "    key_ans = {}\n",
    "\n",
    "    while i < len(src_words):\n",
    "        if src_words[i] == tar_words[j]:\n",
    "            src.append(src_words[i])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            p = i + 1\n",
    "            q = j + 1\n",
    "\n",
    "            while p < len(src_words):\n",
    "                while q < len(tar_words) and tar_words[q] != src_words[p]:\n",
    "                    q += 1\n",
    "                if q == len(tar_words):\n",
    "                    src.append(src_words[p])\n",
    "                    p = p + 1\n",
    "                    q = j + 1\n",
    "                else:\n",
    "                    break\n",
    "            if p - i == 1:\n",
    "                pre = src_words[i]\n",
    "                aft = \" \".join(tar_words[j:q])\n",
    "                if pre in abbrs.keys():\n",
    "                    pass\n",
    "                elif pre.upper() in abbrs.keys():\n",
    "                    pre = pre.upper()\n",
    "                elif pre.lower() in abbrs.keys():\n",
    "                    pre = pre.lower()\n",
    "\n",
    "                if pre in abbrs.keys():\n",
    "                    if len(abbrs[pre]) > 1:\n",
    "                        temp = [' '.join(src_words), 'what is {} ?'.format(pre)]\n",
    "                        label = -1\n",
    "                        skip_cnt = 0\n",
    "                        for index, u in enumerate(abbrs[pre]):\n",
    "                            if index-skip_cnt>=max_pad_length-2:\n",
    "                                break\n",
    "                            if len(u.split(' ')) > 10:\n",
    "                                skip_cnt += 1\n",
    "                                continue\n",
    "                            h = u\n",
    "                            temp.append(h)\n",
    "                            if is_similar(u, aft):\n",
    "                                label = index\n",
    "                        while len(temp) < max_pad_length:\n",
    "                            temp.append('[PAD]')\n",
    "                        if len(tokenizer.tokenize(temp[0])) + len(tokenizer.tokenize(temp[1])) + len(\n",
    "                                tokenizer.tokenize(temp[2])) >= max_dcmn_seq_length\\\n",
    "                                or label<0 or label >= max_pad_length - 2:\n",
    "                            src.append(pre)\n",
    "                        else:\n",
    "                            sentences.append(temp)\n",
    "                            labels.append(label)\n",
    "                            keys.append(pre)\n",
    "                            src.append('[MASK]')\n",
    "                            key_ans[pre] = label\n",
    "                    else:\n",
    "                        src.append(abbrs[pre][0])\n",
    "                else:\n",
    "                    src.append(pre)\n",
    "\n",
    "            i = p\n",
    "            j = q\n",
    "    return sentences, labels, src, keys, key_ans\n",
    "\n",
    "\n",
    "def get_dcmn_data_from_step1(src_words, masks, k_a, abbrs, max_pad_length, max_dcmn_seq_length, tokenizer):\n",
    "    sentences = []\n",
    "    src = ['[CLS]']\n",
    "    keys = []\n",
    "    labels = []\n",
    "    for i, mask in enumerate(masks):\n",
    "        if mask == 0:\n",
    "            src.append(src_words[i])\n",
    "            continue\n",
    "        key = src_words[i]\n",
    "        if key in abbrs.keys() and key in k_a.keys() and k_a[key]>=0 and k_a[key]<max_pad_length-2:\n",
    "            if len(abbrs[key]) > 1:\n",
    "                temp = [' '.join(src_words), 'what is {} ?'.format(key)]\n",
    "                label = -1\n",
    "                if key in k_a.keys():\n",
    "                    label = k_a[key]\n",
    "                skip_cnt = 0\n",
    "                for index, u in enumerate(abbrs[key]):\n",
    "                    if index-skip_cnt >= max_pad_length-2:\n",
    "                        break\n",
    "                    if len(u.split(' ')) > 10:\n",
    "                        skip_cnt += 1\n",
    "                        continue\n",
    "                    h = u\n",
    "                    temp.append(h)\n",
    "\n",
    "                while len(temp) < max_pad_length:\n",
    "                    temp.append('[PAD]')\n",
    "\n",
    "                if len(tokenizer.tokenize(temp[0])) + len(tokenizer.tokenize(temp[1])) + len(\n",
    "                        tokenizer.tokenize(temp[2])) >= max_dcmn_seq_length:\n",
    "                    src.append(key)\n",
    "                    continue\n",
    "                sentences.append(temp)\n",
    "                keys.append(key)\n",
    "                src.append('[SEP] [MASK] [SEP]')\n",
    "                labels.append(label)\n",
    "            else:\n",
    "                src.append(abbrs[key][0])\n",
    "        elif key in abbrs.keys() and key not in k_a.keys() and len(abbrs[key]) == 1:\n",
    "            src.append(abbrs[key][0])\n",
    "        else:\n",
    "            src.append(key)\n",
    "\n",
    "    return sentences, labels, src, keys\n",
    "\n",
    "def add_sep(train_srcs, train_tars):\n",
    "    train_src_new = []\n",
    "    train_tar_new = []\n",
    "\n",
    "    for src, tar in zip(train_srcs, train_tars):\n",
    "        src = ' '.join(src)\n",
    "        src = src.split(' ')\n",
    "        src_new = src\n",
    "        if src_new[-1] != '.':\n",
    "            src_new.append('.')\n",
    "        tar = tar.split(' ')\n",
    "        tar_new = []\n",
    "        p = 0\n",
    "\n",
    "        for i, u in enumerate(src):\n",
    "            if u == '[MASK]':\n",
    "                while p < len(tar) and tar[p] != src[i - 1]:\n",
    "                    tar_new.append(tar[p])\n",
    "                    p += 1\n",
    "                if p < len(tar):\n",
    "                    tar_new.append(tar[p])\n",
    "                    p += 1\n",
    "                tar_new.append('[SEP]')\n",
    "                while p < len(tar) and tar[p] != src[i + 1]:\n",
    "                    tar_new.append(tar[p])\n",
    "                    p += 1\n",
    "                tar_new.append('[SEP]')\n",
    "        while p < len(tar):\n",
    "            tar_new.append(tar[p])\n",
    "            p += 1\n",
    "        train_src_new.append(src_new)\n",
    "        train_tar_new.append(tar_new)\n",
    "\n",
    "    for i, u in enumerate(train_src_new):\n",
    "        tmp = []\n",
    "        for j, v in enumerate(u):\n",
    "            if v == '[MASK]':\n",
    "                tmp.append('[SEP]')\n",
    "                tmp.append('[MASK]')\n",
    "                tmp.append('[SEP]')\n",
    "            else:\n",
    "                tmp.append(v)\n",
    "        train_src_new[i] = tmp\n",
    "    train_input = []\n",
    "    for u in train_src_new:\n",
    "        train_input.append(' '.join(u))\n",
    "    train_output = []\n",
    "    for u in train_tar_new:\n",
    "        train_output.append(' '.join(u))\n",
    "\n",
    "    return train_input, train_output\n",
    "\n",
    "def get_embs(dcmn_keys, abbrs, max_pad_length):\n",
    "    device = torch.device('cuda')\n",
    "    bert_model = 'bert-base-cased'\n",
    "    bert = BertModel.from_pretrained(bert_model)\n",
    "    bert.to(device)\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)\n",
    "    pad_tokens =['[PAD]']\n",
    "    ids = tokenizer.convert_tokens_to_ids(pad_tokens)\n",
    "    inputs = [ids]\n",
    "    inputs = torch.tensor(inputs)\n",
    "    inputs = inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        _, pad_embs = bert(inputs)\n",
    "    pad_embs = pad_embs.cpu().detach().numpy()\n",
    "    dcmn_embs = []\n",
    "    for keys in tqdm(dcmn_keys):\n",
    "        key_embs = []\n",
    "        for key in keys:\n",
    "            emb_values = []\n",
    "            skip_cnt = 0\n",
    "            for i, value in enumerate(abbrs[key]):\n",
    "                if len(value.split(' ')) > 10:\n",
    "                    skip_cnt += 1\n",
    "                    continue\n",
    "                if i - skip_cnt >= max_pad_length - 2:\n",
    "                    break\n",
    "                tokens = tokenizer.tokenize(key)\n",
    "                ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                inputs = [ids]\n",
    "                inputs = torch.tensor(inputs)\n",
    "                inputs = inputs.to(device)\n",
    "                with torch.no_grad():\n",
    "                    _, embs = bert(inputs)\n",
    "                    emb_values.append(embs.cpu().detach().numpy())\n",
    "            while len(emb_values) < max_pad_length-2:\n",
    "                emb_values.append(pad_embs)\n",
    "            key_embs.append(emb_values)\n",
    "        dcmn_embs.append(key_embs)\n",
    "    return dcmn_embs\n",
    "\n",
    "\n",
    "def seq_tokenize(input_data, tokenizer, max_seq_length):\n",
    "    ids = []\n",
    "    for data in tqdm(input_data):\n",
    "        words = tokenizer.tokenize(data)\n",
    "        ids.append(words)\n",
    "\n",
    "    ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in ids],\n",
    "                        maxlen=max_seq_length, dtype=\"long\", value=0,\n",
    "                        truncating=\"post\", padding=\"post\")\n",
    "    masks = [[float(i != 0.0) for i in ii] for ii in ids]\n",
    "    return ids, masks\n",
    "\n",
    "\n",
    "def get_index(srcs, batch_size):\n",
    "    tar_indexs = []\n",
    "    for i, src in enumerate(srcs):\n",
    "        p = 0\n",
    "        indexs = []\n",
    "        for j, u in enumerate(src):\n",
    "            if u == 4:\n",
    "                indexs.append((i%batch_size,j))\n",
    "        tar_indexs.append(indexs)\n",
    "    return tar_indexs\n",
    "\n",
    "\n",
    "def softmax(x, axis=1):\n",
    "    row_max = x.max(axis=axis)\n",
    "\n",
    "    row_max = row_max.reshape(-1, 1)\n",
    "    x = x - row_max\n",
    "\n",
    "    x_exp = np.exp(x)\n",
    "    x_sum = np.sum(x_exp, axis=axis, keepdims=True)\n",
    "    s = x_exp / x_sum\n",
    "    return s\n",
    "\n",
    "class DataGenerator():\n",
    "    def __init__(self, seq_batch_size, max_pad_length=16, max_seq_length=64, cuda=True):\n",
    "        if cuda:\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "\n",
    "        self.abbrs_path = './data/abbrs-all-cased.pkl'\n",
    "        self.train_txt_path = './data/train(12809).txt'\n",
    "        self.test_txt_path = './data/test(2030).txt'\n",
    "        with open(self.abbrs_path, 'rb') as f:\n",
    "            self.abbrs = pickle.load(f)\n",
    "        self.train_src_txt, self.train_tar_1_txt, self.train_tar_2_txt = get_train_src_tar_txt(self.train_txt_path)\n",
    "        # self.train_src_txt = self.train_src_txt[:500]\n",
    "        # self.train_tar_1_txt = self.train_tar_1_txt[:500]\n",
    "        # self.train_tar_2_txt = self.train_tar_2_txt[:500]\n",
    "\n",
    "        self.test_src_txt, self.test_tar_1_txt, self.test_tar_2_txt = get_test_src_tar_txt(self.test_txt_path)\n",
    "\n",
    "        # generate data\n",
    "        self.train_order = []\n",
    "        self.train_seq_srcs = []\n",
    "        self.train_dcmn_srcs = []\n",
    "        self.train_dcmn_labels = []\n",
    "        self.train_keys = []\n",
    "        self.test_order = []\n",
    "        self.test_seq_srcs = []\n",
    "        self.test_dcmn_srcs = []\n",
    "        self.test_dcmn_labels = []\n",
    "        self.test_keys=[]\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "        for i, (src, tar) in enumerate(zip(self.train_src_txt, self.train_tar_1_txt)):\n",
    "            src = nltk.word_tokenize(src)\n",
    "            tar = nltk.word_tokenize(tar)\n",
    "            sentences, labels, src, keys, key_ans = get_dcmn_data_from_gt(src, tar, self.abbrs, max_pad_length=max_pad_length,\n",
    "                                                                    max_dcmn_seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "            self.train_order.append(len(sentences))\n",
    "            self.train_dcmn_srcs.extend(sentences)\n",
    "            self.train_dcmn_labels.extend(labels)\n",
    "            self.train_seq_srcs.append(src)\n",
    "            self.train_keys.append(keys)\n",
    "\n",
    "        with open('./data/test_mask_step2_2030.pkl', 'rb') as f:\n",
    "            test_mask_step2 = pickle.load(f)\n",
    "        test_mask = []\n",
    "\n",
    "        for src, mask in zip(self.test_src_txt, test_mask_step2):\n",
    "            src = nltk.word_tokenize(src)\n",
    "            mask_new = merge_mask(src, mask, tokenizer)\n",
    "            test_mask.append(mask_new)\n",
    "\n",
    "        k_a = []\n",
    "        for i, (src, tar) in enumerate(zip(self.test_src_txt, self.test_tar_1_txt)):\n",
    "            src = nltk.word_tokenize(src)\n",
    "            tar = nltk.word_tokenize(tar)\n",
    "            sentences, labels, src, keys, key_ans = get_dcmn_data_from_gt(src, tar, self.abbrs, max_pad_length=max_pad_length,\n",
    "                                                                          max_dcmn_seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "            k_a.append(key_ans)\n",
    "\n",
    "        for i, (sts, masks, k_a) in enumerate(zip(self.test_src_txt, test_mask, k_a)):\n",
    "            sts = nltk.word_tokenize(sts)\n",
    "            sentences, labels, src, keys = get_dcmn_data_from_step1(sts, masks, k_a, self.abbrs, max_pad_length=max_pad_length,\n",
    "                                                                    max_dcmn_seq_length=max_seq_length, tokenizer=tokenizer)\n",
    "\n",
    "            self.test_keys.append(keys)\n",
    "            self.test_order.append(len(sentences))\n",
    "            self.test_dcmn_srcs.extend(sentences)\n",
    "            self.test_dcmn_labels.extend(labels)\n",
    "            self.test_seq_srcs.append(src)\n",
    "\n",
    "\n",
    "        self.train_seq_srcs, self.train_tar_2_txt = add_sep(train_srcs=self.train_seq_srcs, train_tars=self.train_tar_2_txt)\n",
    "        self.test_seq_srcs = [' '.join(u) for u in self.test_seq_srcs]\n",
    "\n",
    "        # self.train_embs = get_embs(self.train_keys, self.abbrs, max_pad_length)\n",
    "        # self.test_embs = get_embs(self.test_keys, self.abbrs, max_pad_length)\n",
    "        # with open('./data/train_embs.pkl', 'wb') as f:\n",
    "        #     pickle.dump(self.train_embs, f)\n",
    "        # with open('./data/test_embs.pkl', 'wb') as f:\n",
    "        #     pickle.dump(self.test_embs, f)\n",
    "\n",
    "        with open('./data/train_embs.pkl', 'rb') as f:\n",
    "            self.train_embs = pickle.load(f)\n",
    "        with open('./data/test_embs.pkl', 'rb') as f:\n",
    "            self.test_embs = pickle.load(f)\n",
    "\n",
    "        seq_config = Config(16)\n",
    "        seq_tokenizer = seq_config.tokenizer\n",
    "        self.train_seq_srcs_ids, self.train_seq_srcs_masks = seq_tokenize(self.train_seq_srcs, seq_tokenizer,\n",
    "                                                                          max_seq_length)\n",
    "        self.train_seq_tars_ids, self.train_seq_tars_masks = seq_tokenize(self.train_tar_2_txt, seq_tokenizer,\n",
    "                                                                          max_seq_length)\n",
    "        self.test_seq_srcs_ids, self.test_seq_srcs_masks = seq_tokenize(self.test_seq_srcs, seq_tokenizer,\n",
    "                                                                        max_seq_length)\n",
    "        self.cudics = pickle.load(open('./data/test_cudics.pkl', 'rb'))\n",
    "        self.seq_test_tars = pickle.load(open('./data/test_tars.pkl', 'rb'))\n",
    "\n",
    "        self.train_indexes = get_index(self.train_seq_srcs_ids, seq_batch_size)\n",
    "        self.test_indexes = get_index(self.test_seq_srcs_ids, seq_batch_size)\n",
    "\n",
    "        self.p_train, self.p_test = 0, 0\n",
    "        self.q_train, self.q_test = 0, 0\n",
    "        self.p_emb = 0\n",
    "\n",
    "        self.seq_train_data = []    # src_id(64), src_mask(64), tar_id(64), tar_mask(64), sum_new_embs(num of keys in src_id), indexes(index of key)\n",
    "                                    # 0              1            2              3            4                                     5\n",
    "        for src_id, src_mask, tar_id, tar_mask, indexes in zip(self.train_seq_srcs_ids, self.train_seq_srcs_masks,\n",
    "                                                               self.train_seq_tars_ids, self.train_seq_tars_masks,\n",
    "                                                               self.train_indexes):\n",
    "            self.seq_train_data.append([src_id, src_mask, tar_id, tar_mask, [], indexes])\n",
    "\n",
    "        self.seq_test_data = []  # src_id(64), src_mask(64), tars, cudics, sum_new_embs(num of keys in src_id),\n",
    "                                  # 0              1            2     3         4\n",
    "        for src_id, src_mask, tars, cudic in zip(self.test_seq_srcs_ids, self.test_seq_srcs_masks,\n",
    "                                                               self.seq_test_tars, self.cudics):\n",
    "            self.seq_test_data.append([src_id, src_mask, tars, cudic, []])\n",
    "\n",
    "        self.seq_batch_size = seq_batch_size\n",
    "\n",
    "    def train_data_to_tensor(self, datas):\n",
    "        src_ids = torch.LongTensor([_[0] for _ in datas]).to(self.device)\n",
    "        src_masks = torch.LongTensor([_[1] for _ in datas]).to(self.device)\n",
    "        tar_ids = torch.LongTensor([_[2] for _ in datas]).to(self.device)\n",
    "        tar_masks = torch.LongTensor([_[3] for _ in datas]).to(self.device)\n",
    "        sum_new_embs = [_[4] for _ in datas]\n",
    "        indexes = [_[5] for _ in datas]\n",
    "        return src_ids, src_masks, tar_ids, tar_masks, sum_new_embs, indexes\n",
    "\n",
    "\n",
    "    def update_train(self, batch_scores):\n",
    "        batch_scores = softmax(batch_scores, axis=1)\n",
    "        while self.p_train < len(self.train_order) and self.train_order[self.p_train] == 0:\n",
    "            self.p_train += 1\n",
    "\n",
    "        for scores in batch_scores:\n",
    "\n",
    "            new_embs = [score*emb for (score, emb) in zip(scores, self.train_embs[self.p_train][self.p_emb])]\n",
    "            self.p_emb += 1\n",
    "            sum_new_emb = np.sum(new_embs, axis=0)\n",
    "            self.seq_train_data[self.p_train][4].append(sum_new_emb)\n",
    "            if len(self.seq_train_data[self.p_train][4]) == self.train_order[self.p_train]:\n",
    "                self.p_train += 1\n",
    "                while self.p_train < len(self.train_order) and self.train_order[self.p_train] == 0:\n",
    "                    self.p_train += 1\n",
    "                self.p_emb = 0\n",
    "\n",
    "        if self.p_train - self.q_train >= self.seq_batch_size:\n",
    "            batch_data = self.seq_train_data[self.q_train: self.q_train + self.seq_batch_size]\n",
    "            self.q_train += self.seq_batch_size\n",
    "            return self.train_data_to_tensor(batch_data)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def restart_train(self):\n",
    "        self.p_train = 0\n",
    "        self.q_train = 0\n",
    "        self.p_emb = 0\n",
    "        for i in range(len(self.seq_train_data)):\n",
    "            self.seq_train_data[i][4] = []\n",
    "\n",
    "    def update_test(self, batch_scores):\n",
    "        batch_scores = softmax(batch_scores, axis=1)\n",
    "        while self.p_test < len(self.test_order) and self.test_order[self.p_test] == 0:\n",
    "            self.p_test += 1\n",
    "\n",
    "        for scores in batch_scores:\n",
    "            new_embs = [score * emb for (score, emb) in zip(scores, self.test_embs[self.p_test][self.q_test])]\n",
    "            sum_new_emb = np.sum(new_embs, axis=0)\n",
    "            self.seq_test_data[self.p_test][4].append(sum_new_emb)\n",
    "            if len(self.seq_test_data[self.p_test][4]) == self.test_order[self.p_test]:\n",
    "                self.p_test += 1\n",
    "\n",
    "    def restart_test(self):\n",
    "        self.p_test = 0\n",
    "        for i in range(len(self.seq_test_data)):\n",
    "            self.seq_test_data[i][4] = []\n",
    "\n",
    "\n",
    "    def get_test_sum_embs(self):\n",
    "        return [_[4] for _ in self.seq_test_data]\n",
    "\n",
    "    def build_dataset_eval(self):\n",
    "        token_ids_srcs = self.test_seq_srcs_ids\n",
    "        seq_len_src = 64\n",
    "        mask_srcs = self.test_seq_srcs_masks\n",
    "\n",
    "        cudics = self.cudics\n",
    "        test_tars = self.seq_test_tars\n",
    "        test_data = []\n",
    "        for token_ids_src, mask_src, test_tar, cudic in zip(token_ids_srcs, mask_srcs, test_tars, cudics):\n",
    "            tars = test_tar\n",
    "            test_data.append((token_ids_src, int(0), seq_len_src, mask_src, tars, cudic))\n",
    "        return test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:02<00:00, 4399.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12801/12801 [00:03<00:00, 3730.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2031/2031 [00:00<00:00, 4402.98it/s]\n"
     ]
    }
   ],
   "source": [
    "dg = DataGenerator(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11474\n",
      "1708\n",
      "5574 7227 0.4354347316615889\n",
      "916 1115 0.45100935499753814\n"
     ]
    }
   ],
   "source": [
    "print(len(dg.train_dcmn_srcs))\n",
    "print(len(dg.test_dcmn_srcs))\n",
    "tot = 0\n",
    "for u in dg.train_order:\n",
    "    if u == 0:\n",
    "        tot += 1\n",
    "print(tot, len(dg.train_seq_srcs_ids)-tot, tot/len(dg.train_seq_srcs_ids))\n",
    "tot = 0\n",
    "for u in dg.test_order:\n",
    "    if u == 0:\n",
    "        tot += 1\n",
    "print(tot, len(dg.test_seq_srcs_ids)-tot, tot/len(dg.test_seq_srcs_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
